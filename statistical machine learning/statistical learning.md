# Statistical Learning

## 1.统计学习先验知识

### 1.1相关定义

**输入空间**：将输入所有可能的取值的集合

**输出空间**：输出所有可能的取值的集合

**特征空间**：所以特征向量存在的空间，每一维对应于一个特征

**假设空间**：模型将输入到输出空间进行映射 ，该映射的集合就是假设空间。通常用$F$​表示，$F$通常可以定义为决策函数的集合和条件概率的集合
$$
F = {\{f|y = f(x)\}} 决策函数\\
F = {\{P|P(Y|X)\}} 条件概率\\
$$
$F$通常是由一个参数向量$\theta $决定的函数族或者分布族

参数$\theta$​ 取值与$n$​维欧式空间也叫**参数空间**

模型分为概率模型(条件概率$P(X|Y)$)和非概率模型(决策函数$Y = f(x)$)

### 1.2联合概率分布

对于随机变量$x,y$ 

联合概率$P(X，Y) = {S_{X \cap Y} \over S_{all}}$：表示在$x=X，y=Y$条件下的概率

条件概率$P(X|Y) = {S_{X \cap Y} \over S_Y}$：表示在$y=Y条件下x=X$的概率

贝叶斯公式：$P(X,Y) = P(X|Y)*P(Y)$

条件和结果可以互换：
$$
P(X,Y) = P(Y,X) \Rightarrow P(X|Y)*P(Y) = P(Y|X) *P(X) \tag 1
$$
对于$(1)$​只需要知道其中三个因子就能求出第四个，对上式做变形即可得到贝叶斯公式
$$
P(X|Y) = {P(Y|X) * P(X) \over P(Y)} \tag 2
$$

### 1.3决策

损失函数用来衡量模型的预测的好坏，模型越准确预测的值与真实的差距越小

#### 1.3.1损失函数

##### 0-1损失

$$
L(Y,f(X)) = \begin{cases}
0, & Y=f(X) \\
1, & Y \neq f(X) \\
\end{cases} \tag 1
$$

##### 平方损失函数

$$
L(Y,f(X)) = (Y-f(X))^2 \tag 2
$$

##### 绝对损失函数

$$
L(Y,f(X)) = |Y-f(X)| \tag 3
$$

##### 对数损失

$$
L(Y,P(Y|X)) = -logP(Y|X) \tag 4
$$

其中$(1),(2),(3)$​是衡量决策函数损失的，$(4)$​是衡量概率函数损失的

###### 期望(均值)：反应随机变量均值的指标

对于离散型变量$X = x_1,x_2,x_3,\ldots x_n$ 分别对应的概率为$Y = p(x_1),p(x_2),\ldots,p(x_n)$

那么随机变量$X$的期望$E(X) = \sum_{i=1}^n x_{i}p(x_i)$

对于连续型变量$X$的概率密度函数为 $f(X)$ 那么期望为$E(x) = \int_{-\infty}^{\infty} Xf(X)$

所以损失函数的期望是：
$$
R_{exp} = E_{p}[L(Y,f(X))] = \int_{X,Y} L(y,f(x)) p(x,y) dxdy \tag 5
$$
上述式子$(5)$ 即为**风险函数/期望损失** 

学习的目标就是选择**期望风险最小**的模型

**经验风险/经验损失**：训练数据集的平均损失
$$
R_{emp}(f) = {1 \over N} \sum_{i=1}^NL(y_i,f(x_i)) \tag 6
$$
当样本数量足够多的时候，更具大数定律$R_{emp}(f) \rightarrow R_{exp}$

#### 1.3.2 经验风险最小化和结构风险最小化

**[极大似然估计](https://blog.csdn.net/u011508640/article/details/72815981)**：求得一组模型参数使得当前观测到的事件发生的概率最大。求得参数$\theta $ 使得事件$x_i$发生的概率函数(似然函数)：$p(x_i | \theta)$ 最大。 $x_i 表示观测的事件$。

更具已知的实验结果来推测导致这种结果最可能的参数。例如十次掷硬币得到十次正面向上，那么我们可以反推该硬币掷出去正面向上的概率大于0.5

求解极大似然估计：

1. 写出似然函数
2. 对似然函数取对数
3. 计算对数似然函数的极值，求导令导数为0
4. 解释然方差既可以得到最可能参数

例如，对于一枚硬币十次投掷得到7次正面向上，3次向下，称为事件A

那么我们设掷硬币是满足二项分布 投掷向上的概率为 $ \theta $。似然函数--也就是事件A发生的概率：$p(\theta) = \theta^7(1-\theta)^3 $

求极值可得$ \theta = 0.7$ 那么这个参数的最大可能就是$0.7$

**极大后验估计(贝叶斯统计学派)**：极大后验估计是考虑了事件的先验概率。求得一组模型参数使得当前观测到的事件发生的概率和这组参数出现的概率同样最大。即求得参数$\theta $ 使得似然函数$p(x_i | \theta) p(\theta)$ 最大 $x_i 表示观测的事件$ 

因为有贝叶斯公式 
$$
p(\theta | x_i) =  {p(x_i | \theta) p(\theta) \over p(x_i)}
$$
其中$p(x_i)$是更具观测到的样本可以得到的。所以可以看做求得参数$\theta $最大化  后验概率$p(\theta | x_i)$

**经验风险最小化**：经验风险最小的模型是最优模型。即在假设空间寻找一个函数模型能够是经验风险最小，那么我们也认为这个模型是最优模型
$$
\min R_{emp} = \min_{f \in F} {1 \over N} \sum_{i=1}^NL(y_i,f(x_i))
$$
当模型是条件概率分布，损失是对数损失的时候，经验风险最小化就等价于极大似然估计

**结构风险最小化**：防止在样本数量少的情况下，经验风险最小化过拟合
$$
\min R_{srm} = \min_{f \in F} {1 \over N} \sum_{i=1}^NL(y_i,f(x_i)) + \lambda J(f)
$$
后面的$J(f)$为衡量模型复杂度的函数

当模型是条件概率分布，损失是对数损失，模型复杂度由模型先验概率表示的时候。结构风险最小化就等价于最大后验估计

